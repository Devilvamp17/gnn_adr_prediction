{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import gzip \n",
    "import torch\n",
    "import numpy as np \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_file_path = 'data/meddra_all_se.tsv'\n",
    "sider_names_file_path = 'data/drug_names.tsv'\n",
    "sider_df = pd.read_csv(sider_file_path,sep='\\t',header=None, compression=None)\n",
    "drug_names_df = pd.read_csv(sider_names_file_path,sep='\\t',header=None,compression=None)\n",
    "drug_names_df.columns = ['STITCH_flat', 'Drug_Name']\n",
    "sider_df.columns = [\n",
    "    'STITCH_compound_flat',  # Example: CID100000085\n",
    "    'STITCH_compound_stereo',  # Example: CID000010917\n",
    "    'UMLS_concept_id',         # Example: C0000729\n",
    "    'MedDRA_type',             # e.g., LLT\n",
    "    'MedDRA_concept_id',       # Example: C0000729\n",
    "    'LLT_preferred_term'       # e.g., \"Abdominal cramps\"\n",
    "]\n",
    "sider_df.dropna(subset=['MedDRA_type', 'MedDRA_concept_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctd_dir='data'\n",
    "ctd_chem_disease_file = os.path.join(ctd_dir, 'CTD_chemicals_diseases.csv.gz')\n",
    "ctd_chem_gene_file = os.path.join(ctd_dir, 'CTD_chem_gene_ixns.csv.gz')\n",
    "ctd_chemicals_file = os.path.join(ctd_dir, 'CTD_chemicals.csv.gz')\n",
    "ctd_genes_file = os.path.join(ctd_dir, 'CTD_genes.csv.gz')\n",
    "ctd_chem_disease_df = pd.read_csv(\n",
    "        ctd_chem_disease_file,\n",
    "        comment='#', \n",
    "        compression='gzip'\n",
    "    )\n",
    "ctd_chem_gene_df = pd.read_csv(\n",
    "    ctd_chem_gene_file,\n",
    "    comment='#',\n",
    "    compression='gzip'\n",
    ")\n",
    "ctd_chemicals_df = pd.read_csv(\n",
    "    ctd_chemicals_file,\n",
    "    sep='\\t',\n",
    "    comment='#',\n",
    "    compression='gzip'\n",
    ")\n",
    "ctd_genes_df = pd.read_csv(\n",
    "    ctd_genes_file,\n",
    "    sep='\\t',\n",
    "    comment='#',\n",
    "    compression='gzip'\n",
    ")\n",
    "ctd_chem_gene_df.columns = [\n",
    "    \"ChemicalName\",       # e.g., 10074-G5\n",
    "    \"ChemicalID\",         # e.g., C534883\n",
    "    \"CasRN\",              # Unnamed or CAS Registry Number\n",
    "    \"GeneSymbol\",         # e.g., AR\n",
    "    \"GeneID\",             # e.g., 367\n",
    "    \"GeneForms\",          # e.g., protein\n",
    "    \"Organism\",           # e.g., Homo sapiens\n",
    "    \"OrganismID\",         # e.g., 9606\n",
    "    \"Interaction\",        # Natural language interaction\n",
    "    \"InteractionActions\", # Parsed actions e.g., decreases^reaction|increases^expression\n",
    "    \"PubMedIDs\"           # Supporting publication IDs\n",
    "]\n",
    "ctd_chem_disease_df.columns = [\n",
    "    \"ChemicalName\", \"ChemicalID\", \"CasRN\", \"DiseaseName\", \"DiseaseID\", \n",
    "    \"DirectEvidence\", \"InferenceGeneSymbol\", \"InferenceScore\", \n",
    "    \"OmimIDs\", \"PubMedIDs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67341/1001276285.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ctd_chem_disease_df['InferenceScore'].fillna(ctd_chem_disease_df['InferenceScore'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "ctd_chem_disease_df.drop(columns=['CasRN', 'DirectEvidence', 'OmimIDs'],inplace=True)\n",
    "ctd_chem_gene_df.drop(\n",
    "    columns=['CasRN', 'GeneForms', 'Organism', 'OrganismID'],inplace=True\n",
    ")\n",
    "ctd_chem_disease_df['InferenceScore'].fillna(ctd_chem_disease_df['InferenceScore'].mean(), inplace=True)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "ctd_chem_disease_df['InferenceScore'] = scaler.fit_transform(ctd_chem_disease_df[['InferenceScore']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts:\n",
      "  Drugs: 19275\n",
      "  Side Effects: 6060\n",
      "  Diseases: 7281\n",
      "  Genes: 56367\n"
     ]
    }
   ],
   "source": [
    "all_drug_ids = pd.concat([\n",
    "    sider_df['STITCH_compound_flat'], \n",
    "    ctd_chem_gene_df['ChemicalID'], \n",
    "    ctd_chem_disease_df['ChemicalID']\n",
    "]).dropna().unique()\n",
    "all_se_ids = sider_df['MedDRA_concept_id'].dropna().unique()\n",
    "all_disease_ids = ctd_chem_disease_df['DiseaseID'].dropna().unique()\n",
    "all_gene_ids = ctd_chem_gene_df['GeneID'].dropna().unique()\n",
    "\n",
    "print(f\"Original counts:\")\n",
    "print(f\"  Drugs: {len(all_drug_ids)}\")\n",
    "print(f\"  Side Effects: {len(all_se_ids)}\")\n",
    "print(f\"  Diseases: {len(all_disease_ids)}\")\n",
    "print(f\"  Genes: {len(all_gene_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subsampling drugs:\n",
      "  Original drugs: 19275\n",
      "  Sampled drugs: 1542\n",
      "\n",
      "Number of nodes after subsampling:\n",
      "  Drugs: 1542\n",
      "  Side Effects: 6060\n",
      "  Diseases: 7281\n",
      "  Genes: 56367\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sampling_fraction = 0.08  # Set to e.g. 0.1 for 10% sampling\n",
    "num_drugs = len(all_drug_ids)\n",
    "num_sampled_drugs = int(num_drugs * sampling_fraction)\n",
    "\n",
    "sampled_drug_ids = np.random.choice(all_drug_ids, size=num_sampled_drugs, replace=False)\n",
    "\n",
    "print(f\"\\nSubsampling drugs:\")\n",
    "print(f\"  Original drugs: {num_drugs}\")\n",
    "print(f\"  Sampled drugs: {num_sampled_drugs}\")\n",
    "\n",
    "# Create mappings for sampled drugs and full mappings for other node types\n",
    "drug_mapping = {id: i for i, id in enumerate(sampled_drug_ids)}\n",
    "se_mapping = {id: i for i, id in enumerate(all_se_ids)}\n",
    "disease_mapping = {id: i for i, id in enumerate(all_disease_ids)}\n",
    "gene_mapping = {id: i for i, id in enumerate(all_gene_ids)}\n",
    "\n",
    "print(f\"\\nNumber of nodes after subsampling:\")\n",
    "print(f\"  Drugs: {len(drug_mapping)}\")\n",
    "print(f\"  Side Effects: {len(se_mapping)}\")\n",
    "print(f\"  Diseases: {len(disease_mapping)}\")\n",
    "print(f\"  Genes: {len(gene_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded drug features (sample): [1304   10 1393 1261  144]\n",
      "Encoded side effect features (sample): [   1    4 4270   58   66]\n",
      "Encoded disease features (sample): [2458 2391 2496 4440 2610]\n",
      "Encoded gene features (sample): [ 277 1480 2938 3193 1702]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "drug_encoder = LabelEncoder()\n",
    "se_encoder = LabelEncoder()\n",
    "disease_encoder = LabelEncoder()\n",
    "gene_encoder = LabelEncoder()\n",
    "drug_encoder.fit(sampled_drug_ids)\n",
    "encoded_drug_features = drug_encoder.transform(sampled_drug_ids)\n",
    "\n",
    "# Encode side effects (you can use all SEs here since subsampling is only on drugs)\n",
    "encoded_se_features = se_encoder.fit_transform(all_se_ids)\n",
    "\n",
    "# Encode diseases and genes (same reasoning applies as side effects)\n",
    "encoded_disease_features = disease_encoder.fit_transform(all_disease_ids)\n",
    "encoded_gene_features = gene_encoder.fit_transform(all_gene_ids)\n",
    "\n",
    "# Step 4: Check encoded features for drugs, side effects, diseases, and genes\n",
    "print(f\"\\nEncoded drug features (sample): {encoded_drug_features[:5]}\")\n",
    "print(f\"Encoded side effect features (sample): {encoded_se_features[:5]}\")\n",
    "print(f\"Encoded disease features (sample): {encoded_disease_features[:5]}\")\n",
    "print(f\"Encoded gene features (sample): {encoded_gene_features[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered edges counts:\n",
      "  SIDER drug-side effect edges: 25969\n",
      "  CTD chemical-disease edges: 683718\n",
      "  CTD chemical-gene edges: 188412\n"
     ]
    }
   ],
   "source": [
    "sider_df_sampled = sider_df[sider_df['STITCH_compound_flat'].isin(sampled_drug_ids)].copy()\n",
    "ctd_chem_disease_df_sampled = ctd_chem_disease_df[ctd_chem_disease_df['ChemicalID'].isin(sampled_drug_ids)].copy()\n",
    "ctd_chem_gene_df_sampled = ctd_chem_gene_df[ctd_chem_gene_df['ChemicalID'].isin(sampled_drug_ids)].copy()\n",
    "\n",
    "print(f\"\\nFiltered edges counts:\")\n",
    "print(f\"  SIDER drug-side effect edges: {len(sider_df_sampled)}\")\n",
    "print(f\"  CTD chemical-disease edges: {len(ctd_chem_disease_df_sampled)}\")\n",
    "print(f\"  CTD chemical-gene edges: {len(ctd_chem_gene_df_sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug-Side Effect edges: 25969\n",
      "Drug-Disease edges: 683718\n",
      "Drug-Gene edges: 188412\n"
     ]
    }
   ],
   "source": [
    "drug_mapping = {drug_id: idx for idx, drug_id in enumerate(drug_encoder.classes_)}\n",
    "se_mapping = {se_id: idx for idx, se_id in enumerate(se_encoder.classes_)}\n",
    "disease_mapping = {disease_id: idx for idx, disease_id in enumerate(disease_encoder.classes_)}\n",
    "gene_mapping = {gene_id: idx for idx, gene_id in enumerate(gene_encoder.classes_)}\n",
    "\n",
    "# Drug -> Side Effect edges\n",
    "sider_edges = []\n",
    "for _, row in sider_df_sampled.iterrows():\n",
    "    drug_id = row['STITCH_compound_flat']\n",
    "    se_id = row['MedDRA_concept_id']\n",
    "    if drug_id in drug_mapping and se_id in se_mapping:\n",
    "        sider_edges.append((drug_mapping[drug_id], se_mapping[se_id]))\n",
    "\n",
    "sider_edge_index = torch.tensor(sider_edges, dtype=torch.long).t().contiguous()\n",
    "print(f\"Drug-Side Effect edges: {sider_edge_index.size(1)}\")\n",
    "\n",
    "# Drug -> Disease edges\n",
    "ctd_chem_disease_edges = []\n",
    "for _, row in ctd_chem_disease_df_sampled.iterrows():\n",
    "    chem_id = row['ChemicalID']\n",
    "    disease_id = row['DiseaseID']\n",
    "    if chem_id in drug_mapping and disease_id in disease_mapping:\n",
    "        ctd_chem_disease_edges.append((drug_mapping[chem_id], disease_mapping[disease_id]))\n",
    "\n",
    "ctd_chem_disease_edge_index = torch.tensor(ctd_chem_disease_edges, dtype=torch.long).t().contiguous()\n",
    "print(f\"Drug-Disease edges: {ctd_chem_disease_edge_index.size(1)}\")\n",
    "\n",
    "# Drug -> Gene edges\n",
    "ctd_chem_gene_edges = []\n",
    "for _, row in ctd_chem_gene_df_sampled.iterrows():\n",
    "    chem_id = row['ChemicalID']\n",
    "    gene_id = row['GeneID']\n",
    "    if chem_id in drug_mapping and gene_id in gene_mapping:\n",
    "        ctd_chem_gene_edges.append((drug_mapping[chem_id], gene_mapping[gene_id]))\n",
    "\n",
    "ctd_chem_gene_edge_index = torch.tensor(ctd_chem_gene_edges, dtype=torch.long).t().contiguous()\n",
    "print(f\"Drug-Gene edges: {ctd_chem_gene_edge_index.size(1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.calibration import LabelEncoder\n",
    "num_drug_features = 100\n",
    "num_se_features = 80\n",
    "num_disease_features = 90\n",
    "num_gene_features = 150\n",
    "\n",
    "x_drug = torch.randn(len(drug_mapping), num_drug_features)\n",
    "x_side_effect = torch.randn(len(se_mapping), num_se_features)\n",
    "x_disease = torch.randn(len(disease_mapping), num_disease_features)\n",
    "x_gene = torch.randn(len(gene_mapping), num_gene_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "\n",
    "class HeterogeneousGNNModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.edge_types = [\n",
    "            ('drug', 'causes', 'side_effect'),\n",
    "            ('side_effect', 'is_caused_by', 'drug'),\n",
    "            ('drug', 'associates', 'disease'),\n",
    "            ('disease', 'is_associated_with_drug', 'drug'),\n",
    "            ('drug', 'interacts', 'gene'),\n",
    "            ('gene', 'is_interacted_by', 'drug'),\n",
    "        ]\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "            for edge_type in self.edge_types\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: GATConv((hidden_channels, hidden_channels), hidden_channels, add_self_loops=False)\n",
    "            for edge_type in self.edge_types\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.dropout1(x) for key, x in x_dict.items()}\n",
    "\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.dropout2(x) for key, x in x_dict.items()}\n",
    "\n",
    "        return x_dict\n",
    "\n",
    "\n",
    "# --- 10. Link Prediction Head ---\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, z_drug, z_side_effect):\n",
    "        z = torch.cat([z_drug, z_side_effect], dim=-1) # Example: Concatenation\n",
    "        return self.lin(z)\n",
    "\n",
    "\n",
    "# --- 11. Helper function for Link Prediction Metrics ---\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def get_link_prediction_metrics(pos_pred, neg_pred):\n",
    "    pos_prob = torch.sigmoid(pos_pred)\n",
    "    neg_prob = torch.sigmoid(neg_pred)\n",
    "\n",
    "    probs = torch.cat([pos_prob, neg_prob], dim=0).detach().cpu().numpy()\n",
    "    ground_truth = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)], dim=0).detach().cpu().numpy()\n",
    "\n",
    "    auc_score = roc_auc_score(ground_truth, probs)\n",
    "    auprc_score = average_precision_score(ground_truth, probs)\n",
    "\n",
    "    return auc_score, auprc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "data = HeteroData()\n",
    "\n",
    "data['drug'].x = x_drug\n",
    "data['side_effect'].x = x_side_effect\n",
    "data['disease'].x = x_disease\n",
    "data['gene'].x = x_gene\n",
    "\n",
    "data['drug', 'causes', 'side_effect'].edge_index = sider_edge_index\n",
    "data['drug', 'associates', 'disease'].edge_index = ctd_chem_disease_edge_index\n",
    "data['drug', 'interacts', 'gene'].edge_index = ctd_chem_gene_edge_index\n",
    "data['side_effect', 'is_caused_by', 'drug'].edge_index = sider_edge_index.flip(0)\n",
    "data['disease', 'is_associated_with_drug', 'drug'].edge_index = ctd_chem_disease_edge_index.flip(0)\n",
    "data['gene', 'is_interacted_by', 'drug'].edge_index = ctd_chem_gene_edge_index.flip(0)\n",
    "device='cpu'\n",
    "data.to(device)\n",
    "target_edge_index = data['drug', 'causes', 'side_effect'].edge_index\n",
    "\n",
    "# Split positive edges into train, val, test\n",
    "num_target_edges = target_edge_index.size(1)\n",
    "perm = np.random.permutation(num_target_edges)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_train_pos = int(num_target_edges * train_ratio)\n",
    "num_val_pos = int(num_target_edges * val_ratio)\n",
    "num_test_pos = num_target_edges - num_train_pos - num_val_pos\n",
    "\n",
    "train_pos_indices = perm[:num_train_pos]\n",
    "val_pos_indices = perm[num_train_pos:num_train_pos + num_val_pos]\n",
    "test_pos_indices = perm[num_train_pos + num_val_pos:]\n",
    "\n",
    "# Extract the actual edge index tensors for positive edges\n",
    "data['drug', 'causes', 'side_effect'].edge_index_train_pos = target_edge_index[:, train_pos_indices].to(device)\n",
    "data['drug', 'causes', 'side_effect'].edge_index_val_pos = target_edge_index[:, val_pos_indices].to(device)\n",
    "data['drug', 'causes', 'side_effect'].edge_index_test_pos = target_edge_index[:, test_pos_indices].to(device)\n",
    "\n",
    "# Generate negative samples for validation and test sets\n",
    "# Ensure these negative samples are NOT present in the original positive edges\n",
    "num_drug_nodes = data['drug'].num_nodes\n",
    "num_se_nodes = data['side_effect'].num_nodes\n",
    "\n",
    "data['drug', 'causes', 'side_effect'].edge_index_val_neg = torch.randint(\n",
    "    0, num_drug_nodes, (2, num_val_pos), dtype=torch.long, device=device) # Example: same number as val_pos\n",
    "\n",
    "data['drug', 'causes', 'side_effect'].edge_index_test_neg = torch.randint(\n",
    "    0, num_drug_nodes, (2, num_test_pos), dtype=torch.long, device=device) # Example: same number as test_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 64\n",
    "gnn_model = HeterogeneousGNNModel(hidden_channels=hidden_channels, dropout_prob=0.5)\n",
    "link_predictor = LinkPredictor(in_channels=hidden_channels * 2) # in_channels matches how you combine embeddings\n",
    "\n",
    "gnn_model.to(device)\n",
    "link_predictor.to(device)\n",
    "\n",
    "weight_decay_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(list(gnn_model.parameters()) + list(link_predictor.parameters()), lr=0.01, weight_decay=weight_decay_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train_link_prediction():\n",
    "    gnn_model.train()\n",
    "    link_predictor.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get learned node embeddings\n",
    "    z_dict = gnn_model(data.x_dict, data.edge_index_dict)\n",
    "    z_drug = z_dict['drug']\n",
    "    z_side_effect = z_dict['side_effect']\n",
    "\n",
    "    train_pos_edge_index = data['drug', 'causes', 'side_effect'].edge_index_train_pos\n",
    "\n",
    "    num_train_pos = train_pos_edge_index.size(1)\n",
    "    num_drug_nodes_in_graph = data['drug'].num_nodes\n",
    "    num_se_nodes_in_graph = data['side_effect'].num_nodes\n",
    "\n",
    "    train_neg_edge_index = torch.randint(\n",
    "        0, num_drug_nodes_in_graph, (2, num_train_pos), dtype=torch.long, device=device)\n",
    "    train_neg_edge_index[1, :] = torch.randint(\n",
    "        0, num_se_nodes_in_graph, (1, num_train_pos), dtype=torch.long, device=device)\n",
    "\n",
    "    # --- Get Embeddings for Sampled Edges ---\n",
    "    pos_drug_emb = z_drug[train_pos_edge_index[0]]\n",
    "    pos_se_emb = z_side_effect[train_pos_edge_index[1]]\n",
    "\n",
    "    neg_drug_emb = z_drug[train_neg_edge_index[0]]\n",
    "    neg_se_emb = z_side_effect[train_neg_edge_index[1]]\n",
    "\n",
    "    # --- Calculate Link Prediction Scores ---\n",
    "    pos_scores = link_predictor(pos_drug_emb, pos_se_emb)\n",
    "    neg_scores = link_predictor(neg_drug_emb, neg_se_emb)\n",
    "\n",
    "    # --- Calculate Loss ---\n",
    "    scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "    ground_truth = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)], dim=0)\n",
    "\n",
    "    loss = criterion(scores.squeeze(), ground_truth.squeeze())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "def evaluate_link_prediction(pos_edge_index, neg_edge_index):\n",
    "     gnn_model.eval()\n",
    "     link_predictor.eval()\n",
    "     with torch.no_grad():\n",
    "         # Get learned node embeddings from the full graph structure\n",
    "         z_dict = gnn_model(data.x_dict, data.edge_index_dict)\n",
    "         z_drug = z_dict['drug']\n",
    "         z_side_effect = z_dict['side_effect']\n",
    "\n",
    "         # --- Get Embeddings for Evaluation Edges ---\n",
    "         eval_pos_drug_emb = z_drug[pos_edge_index[0]]\n",
    "         eval_pos_se_emb = z_side_effect[pos_edge_index[1]]\n",
    "\n",
    "         eval_neg_drug_emb = z_drug[neg_edge_index[0]]\n",
    "         eval_neg_se_emb = z_side_effect[neg_edge_index[1]]\n",
    "\n",
    "         # --- Calculate Link Prediction Scores for Evaluation Edges ---\n",
    "         eval_pos_scores = link_predictor(eval_pos_drug_emb, eval_pos_se_emb)\n",
    "         eval_neg_scores = link_predictor(eval_neg_drug_emb, eval_neg_se_emb)\n",
    "\n",
    "         # --- Calculate Evaluation Loss ---\n",
    "         eval_scores = torch.cat([eval_pos_scores, eval_neg_scores], dim=0)\n",
    "         eval_ground_truth = torch.cat([torch.ones_like(eval_pos_scores), torch.zeros_like(eval_neg_scores)], dim=0)\n",
    "         eval_loss = criterion(eval_scores.squeeze(), eval_ground_truth.squeeze())\n",
    "\n",
    "         # --- Calculate Evaluation Metrics (AUC, AUPRC) ---\n",
    "         eval_auc, eval_auprc = get_link_prediction_metrics(eval_pos_scores, eval_neg_scores)\n",
    "\n",
    "     return eval_loss.item(), eval_auc, eval_auprc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Link Prediction Training...\n",
      "Epoch: 001, Train Loss: 0.7351, Val Loss: 0.4202, Val AUC: 0.9780, Val AUPRC: 0.9562\n",
      "Epoch: 002, Train Loss: 0.3883, Val Loss: 0.1951, Val AUC: 0.9735, Val AUPRC: 0.9339\n",
      "Epoch: 003, Train Loss: 0.1755, Val Loss: 0.1329, Val AUC: 0.9719, Val AUPRC: 0.9296\n",
      "Epoch: 004, Train Loss: 0.1400, Val Loss: 0.1420, Val AUC: 0.9711, Val AUPRC: 0.9283\n",
      "Epoch: 005, Train Loss: 0.1696, Val Loss: 0.1324, Val AUC: 0.9709, Val AUPRC: 0.9278\n",
      "Epoch: 006, Train Loss: 0.1542, Val Loss: 0.1254, Val AUC: 0.9803, Val AUPRC: 0.9583\n",
      "Epoch: 007, Train Loss: 0.1274, Val Loss: 0.1439, Val AUC: 0.9806, Val AUPRC: 0.9579\n",
      "Epoch: 008, Train Loss: 0.1236, Val Loss: 0.1561, Val AUC: 0.9812, Val AUPRC: 0.9600\n",
      "Epoch: 009, Train Loss: 0.1246, Val Loss: 0.1379, Val AUC: 0.9829, Val AUPRC: 0.9615\n",
      "Epoch: 010, Train Loss: 0.1143, Val Loss: 0.1192, Val AUC: 0.9846, Val AUPRC: 0.9689\n",
      "Epoch: 011, Train Loss: 0.0976, Val Loss: 0.1093, Val AUC: 0.9843, Val AUPRC: 0.9673\n",
      "Epoch: 012, Train Loss: 0.0976, Val Loss: 0.1067, Val AUC: 0.9840, Val AUPRC: 0.9646\n",
      "Epoch: 013, Train Loss: 0.0924, Val Loss: 0.1063, Val AUC: 0.9844, Val AUPRC: 0.9663\n",
      "Epoch: 014, Train Loss: 0.0947, Val Loss: 0.1053, Val AUC: 0.9851, Val AUPRC: 0.9679\n",
      "Epoch: 015, Train Loss: 0.0938, Val Loss: 0.1034, Val AUC: 0.9859, Val AUPRC: 0.9704\n",
      "Epoch: 016, Train Loss: 0.0913, Val Loss: 0.1025, Val AUC: 0.9861, Val AUPRC: 0.9747\n",
      "Epoch: 017, Train Loss: 0.0901, Val Loss: 0.1050, Val AUC: 0.9862, Val AUPRC: 0.9766\n",
      "Epoch: 018, Train Loss: 0.0843, Val Loss: 0.1087, Val AUC: 0.9865, Val AUPRC: 0.9776\n",
      "Epoch: 019, Train Loss: 0.0913, Val Loss: 0.1054, Val AUC: 0.9872, Val AUPRC: 0.9786\n",
      "Epoch: 020, Train Loss: 0.0864, Val Loss: 0.1003, Val AUC: 0.9875, Val AUPRC: 0.9788\n",
      "Epoch: 021, Train Loss: 0.0814, Val Loss: 0.0976, Val AUC: 0.9876, Val AUPRC: 0.9790\n",
      "Epoch: 022, Train Loss: 0.0832, Val Loss: 0.0962, Val AUC: 0.9878, Val AUPRC: 0.9792\n",
      "Epoch: 023, Train Loss: 0.0849, Val Loss: 0.0968, Val AUC: 0.9879, Val AUPRC: 0.9799\n",
      "Epoch: 024, Train Loss: 0.0803, Val Loss: 0.0982, Val AUC: 0.9880, Val AUPRC: 0.9805\n",
      "Epoch: 025, Train Loss: 0.0801, Val Loss: 0.1002, Val AUC: 0.9879, Val AUPRC: 0.9809\n",
      "Epoch: 026, Train Loss: 0.0795, Val Loss: 0.1040, Val AUC: 0.9878, Val AUPRC: 0.9811\n",
      "Epoch: 027, Train Loss: 0.0834, Val Loss: 0.1061, Val AUC: 0.9877, Val AUPRC: 0.9815\n",
      "Epoch: 028, Train Loss: 0.0832, Val Loss: 0.1063, Val AUC: 0.9877, Val AUPRC: 0.9821\n",
      "Epoch: 029, Train Loss: 0.0765, Val Loss: 0.1038, Val AUC: 0.9877, Val AUPRC: 0.9824\n",
      "Epoch: 030, Train Loss: 0.0710, Val Loss: 0.1034, Val AUC: 0.9876, Val AUPRC: 0.9825\n",
      "Epoch: 031, Train Loss: 0.0795, Val Loss: 0.1067, Val AUC: 0.9879, Val AUPRC: 0.9831\n",
      "Epoch: 032, Train Loss: 0.0748, Val Loss: 0.1063, Val AUC: 0.9879, Val AUPRC: 0.9832\n",
      "Epoch: 033, Train Loss: 0.0754, Val Loss: 0.1069, Val AUC: 0.9878, Val AUPRC: 0.9831\n",
      "Epoch: 034, Train Loss: 0.0740, Val Loss: 0.1062, Val AUC: 0.9878, Val AUPRC: 0.9831\n",
      "Epoch: 035, Train Loss: 0.0753, Val Loss: 0.1029, Val AUC: 0.9881, Val AUPRC: 0.9835\n",
      "Epoch: 036, Train Loss: 0.0698, Val Loss: 0.1028, Val AUC: 0.9884, Val AUPRC: 0.9839\n",
      "Epoch: 037, Train Loss: 0.0753, Val Loss: 0.1079, Val AUC: 0.9884, Val AUPRC: 0.9839\n",
      "Epoch: 038, Train Loss: 0.0701, Val Loss: 0.1124, Val AUC: 0.9886, Val AUPRC: 0.9838\n",
      "Epoch: 039, Train Loss: 0.0707, Val Loss: 0.1093, Val AUC: 0.9887, Val AUPRC: 0.9839\n",
      "Epoch: 040, Train Loss: 0.0716, Val Loss: 0.1011, Val AUC: 0.9886, Val AUPRC: 0.9839\n",
      "Epoch: 041, Train Loss: 0.0667, Val Loss: 0.0986, Val AUC: 0.9888, Val AUPRC: 0.9838\n",
      "Epoch: 042, Train Loss: 0.0717, Val Loss: 0.0981, Val AUC: 0.9889, Val AUPRC: 0.9837\n",
      "Epoch: 043, Train Loss: 0.0717, Val Loss: 0.1002, Val AUC: 0.9891, Val AUPRC: 0.9835\n",
      "Epoch: 044, Train Loss: 0.0685, Val Loss: 0.1034, Val AUC: 0.9891, Val AUPRC: 0.9834\n",
      "Epoch: 045, Train Loss: 0.0702, Val Loss: 0.1051, Val AUC: 0.9890, Val AUPRC: 0.9832\n",
      "Epoch: 046, Train Loss: 0.0685, Val Loss: 0.1018, Val AUC: 0.9882, Val AUPRC: 0.9825\n",
      "Epoch: 047, Train Loss: 0.0673, Val Loss: 0.1001, Val AUC: 0.9876, Val AUPRC: 0.9816\n",
      "Epoch: 048, Train Loss: 0.0679, Val Loss: 0.0980, Val AUC: 0.9876, Val AUPRC: 0.9813\n",
      "Epoch: 049, Train Loss: 0.0718, Val Loss: 0.0988, Val AUC: 0.9874, Val AUPRC: 0.9812\n",
      "Epoch: 050, Train Loss: 0.0670, Val Loss: 0.1025, Val AUC: 0.9873, Val AUPRC: 0.9812\n",
      "Epoch: 051, Train Loss: 0.0660, Val Loss: 0.1051, Val AUC: 0.9877, Val AUPRC: 0.9818\n",
      "Epoch: 052, Train Loss: 0.0695, Val Loss: 0.1044, Val AUC: 0.9882, Val AUPRC: 0.9825\n",
      "Epoch: 053, Train Loss: 0.0661, Val Loss: 0.0975, Val AUC: 0.9885, Val AUPRC: 0.9827\n",
      "Epoch: 054, Train Loss: 0.0634, Val Loss: 0.0965, Val AUC: 0.9890, Val AUPRC: 0.9830\n",
      "Epoch: 055, Train Loss: 0.0685, Val Loss: 0.0987, Val AUC: 0.9894, Val AUPRC: 0.9833\n",
      "Epoch: 056, Train Loss: 0.0656, Val Loss: 0.1033, Val AUC: 0.9893, Val AUPRC: 0.9832\n",
      "Epoch: 057, Train Loss: 0.0665, Val Loss: 0.1170, Val AUC: 0.9885, Val AUPRC: 0.9824\n",
      "Epoch: 058, Train Loss: 0.0675, Val Loss: 0.1127, Val AUC: 0.9868, Val AUPRC: 0.9812\n",
      "Epoch: 059, Train Loss: 0.0660, Val Loss: 0.1044, Val AUC: 0.9865, Val AUPRC: 0.9808\n",
      "Epoch: 060, Train Loss: 0.0675, Val Loss: 0.0976, Val AUC: 0.9875, Val AUPRC: 0.9821\n",
      "Epoch: 061, Train Loss: 0.0676, Val Loss: 0.0960, Val AUC: 0.9877, Val AUPRC: 0.9817\n",
      "Epoch: 062, Train Loss: 0.0628, Val Loss: 0.0984, Val AUC: 0.9873, Val AUPRC: 0.9799\n",
      "Epoch: 063, Train Loss: 0.0677, Val Loss: 0.1033, Val AUC: 0.9870, Val AUPRC: 0.9793\n",
      "Epoch: 064, Train Loss: 0.0668, Val Loss: 0.0965, Val AUC: 0.9879, Val AUPRC: 0.9818\n",
      "Epoch: 065, Train Loss: 0.0649, Val Loss: 0.0907, Val AUC: 0.9884, Val AUPRC: 0.9830\n",
      "Epoch: 066, Train Loss: 0.0674, Val Loss: 0.0879, Val AUC: 0.9887, Val AUPRC: 0.9829\n",
      "Epoch: 067, Train Loss: 0.0627, Val Loss: 0.0894, Val AUC: 0.9887, Val AUPRC: 0.9825\n",
      "Epoch: 068, Train Loss: 0.0670, Val Loss: 0.0879, Val AUC: 0.9889, Val AUPRC: 0.9832\n",
      "Epoch: 069, Train Loss: 0.0628, Val Loss: 0.0873, Val AUC: 0.9892, Val AUPRC: 0.9836\n",
      "Epoch: 070, Train Loss: 0.0658, Val Loss: 0.0922, Val AUC: 0.9899, Val AUPRC: 0.9844\n",
      "Epoch: 071, Train Loss: 0.0665, Val Loss: 0.0960, Val AUC: 0.9899, Val AUPRC: 0.9847\n",
      "Epoch: 072, Train Loss: 0.0634, Val Loss: 0.0946, Val AUC: 0.9884, Val AUPRC: 0.9826\n",
      "Epoch: 073, Train Loss: 0.0662, Val Loss: 0.0931, Val AUC: 0.9876, Val AUPRC: 0.9794\n",
      "Epoch: 074, Train Loss: 0.0628, Val Loss: 0.0938, Val AUC: 0.9875, Val AUPRC: 0.9800\n",
      "Epoch: 075, Train Loss: 0.0657, Val Loss: 0.0947, Val AUC: 0.9877, Val AUPRC: 0.9803\n",
      "Epoch: 076, Train Loss: 0.0612, Val Loss: 0.0951, Val AUC: 0.9880, Val AUPRC: 0.9807\n",
      "Epoch: 077, Train Loss: 0.0629, Val Loss: 0.0963, Val AUC: 0.9884, Val AUPRC: 0.9814\n",
      "Epoch: 078, Train Loss: 0.0629, Val Loss: 0.0922, Val AUC: 0.9888, Val AUPRC: 0.9826\n",
      "Epoch: 079, Train Loss: 0.0598, Val Loss: 0.0898, Val AUC: 0.9890, Val AUPRC: 0.9836\n",
      "Epoch: 080, Train Loss: 0.0625, Val Loss: 0.0859, Val AUC: 0.9888, Val AUPRC: 0.9837\n",
      "Epoch: 081, Train Loss: 0.0634, Val Loss: 0.0857, Val AUC: 0.9888, Val AUPRC: 0.9836\n",
      "Epoch: 082, Train Loss: 0.0612, Val Loss: 0.0868, Val AUC: 0.9887, Val AUPRC: 0.9826\n",
      "Epoch: 083, Train Loss: 0.0595, Val Loss: 0.0904, Val AUC: 0.9887, Val AUPRC: 0.9818\n",
      "Epoch: 084, Train Loss: 0.0613, Val Loss: 0.0934, Val AUC: 0.9884, Val AUPRC: 0.9811\n",
      "Epoch: 085, Train Loss: 0.0614, Val Loss: 0.0941, Val AUC: 0.9879, Val AUPRC: 0.9801\n",
      "Epoch: 086, Train Loss: 0.0665, Val Loss: 0.0899, Val AUC: 0.9877, Val AUPRC: 0.9785\n",
      "Epoch: 087, Train Loss: 0.0632, Val Loss: 0.0892, Val AUC: 0.9875, Val AUPRC: 0.9782\n",
      "Epoch: 088, Train Loss: 0.0619, Val Loss: 0.0908, Val AUC: 0.9872, Val AUPRC: 0.9780\n",
      "Epoch: 089, Train Loss: 0.0619, Val Loss: 0.0922, Val AUC: 0.9874, Val AUPRC: 0.9790\n",
      "Epoch: 090, Train Loss: 0.0652, Val Loss: 0.0902, Val AUC: 0.9881, Val AUPRC: 0.9799\n",
      "Epoch: 091, Train Loss: 0.0613, Val Loss: 0.0884, Val AUC: 0.9882, Val AUPRC: 0.9798\n",
      "Epoch: 092, Train Loss: 0.0594, Val Loss: 0.0880, Val AUC: 0.9884, Val AUPRC: 0.9801\n",
      "Epoch: 093, Train Loss: 0.0611, Val Loss: 0.0873, Val AUC: 0.9884, Val AUPRC: 0.9802\n",
      "Epoch: 094, Train Loss: 0.0609, Val Loss: 0.0866, Val AUC: 0.9884, Val AUPRC: 0.9800\n",
      "Epoch: 095, Train Loss: 0.0612, Val Loss: 0.0862, Val AUC: 0.9885, Val AUPRC: 0.9798\n",
      "Epoch: 096, Train Loss: 0.0597, Val Loss: 0.0867, Val AUC: 0.9886, Val AUPRC: 0.9798\n",
      "Epoch: 097, Train Loss: 0.0634, Val Loss: 0.0910, Val AUC: 0.9887, Val AUPRC: 0.9802\n",
      "Epoch: 098, Train Loss: 0.0614, Val Loss: 0.0900, Val AUC: 0.9885, Val AUPRC: 0.9799\n",
      "Epoch: 099, Train Loss: 0.0592, Val Loss: 0.0897, Val AUC: 0.9883, Val AUPRC: 0.9796\n",
      "Epoch: 100, Train Loss: 0.0556, Val Loss: 0.0886, Val AUC: 0.9880, Val AUPRC: 0.9794\n",
      "Epoch: 101, Train Loss: 0.0642, Val Loss: 0.0866, Val AUC: 0.9884, Val AUPRC: 0.9797\n",
      "Epoch: 102, Train Loss: 0.0578, Val Loss: 0.0843, Val AUC: 0.9888, Val AUPRC: 0.9798\n",
      "Epoch: 103, Train Loss: 0.0615, Val Loss: 0.0836, Val AUC: 0.9892, Val AUPRC: 0.9798\n",
      "Epoch: 104, Train Loss: 0.0608, Val Loss: 0.0839, Val AUC: 0.9891, Val AUPRC: 0.9800\n",
      "Epoch: 105, Train Loss: 0.0597, Val Loss: 0.0856, Val AUC: 0.9888, Val AUPRC: 0.9799\n",
      "Epoch: 106, Train Loss: 0.0638, Val Loss: 0.0937, Val AUC: 0.9886, Val AUPRC: 0.9802\n",
      "Epoch: 107, Train Loss: 0.0596, Val Loss: 0.0930, Val AUC: 0.9877, Val AUPRC: 0.9784\n",
      "Epoch: 108, Train Loss: 0.0632, Val Loss: 0.0876, Val AUC: 0.9871, Val AUPRC: 0.9766\n",
      "Epoch: 109, Train Loss: 0.0637, Val Loss: 0.0872, Val AUC: 0.9875, Val AUPRC: 0.9783\n",
      "Epoch: 110, Train Loss: 0.0697, Val Loss: 0.0915, Val AUC: 0.9880, Val AUPRC: 0.9806\n",
      "Epoch: 111, Train Loss: 0.0603, Val Loss: 0.1089, Val AUC: 0.9890, Val AUPRC: 0.9826\n",
      "Epoch: 112, Train Loss: 0.0731, Val Loss: 0.0857, Val AUC: 0.9888, Val AUPRC: 0.9828\n",
      "Epoch: 113, Train Loss: 0.0572, Val Loss: 0.0835, Val AUC: 0.9888, Val AUPRC: 0.9824\n",
      "Epoch: 114, Train Loss: 0.0635, Val Loss: 0.0838, Val AUC: 0.9890, Val AUPRC: 0.9833\n",
      "Epoch: 115, Train Loss: 0.0633, Val Loss: 0.0815, Val AUC: 0.9897, Val AUPRC: 0.9843\n",
      "Epoch: 116, Train Loss: 0.0624, Val Loss: 0.0858, Val AUC: 0.9904, Val AUPRC: 0.9847\n",
      "Epoch: 117, Train Loss: 0.0641, Val Loss: 0.0900, Val AUC: 0.9904, Val AUPRC: 0.9848\n",
      "Epoch: 118, Train Loss: 0.0619, Val Loss: 0.0856, Val AUC: 0.9893, Val AUPRC: 0.9837\n",
      "Epoch: 119, Train Loss: 0.0591, Val Loss: 0.0846, Val AUC: 0.9886, Val AUPRC: 0.9827\n",
      "Epoch: 120, Train Loss: 0.0610, Val Loss: 0.0857, Val AUC: 0.9883, Val AUPRC: 0.9823\n",
      "Epoch: 121, Train Loss: 0.0665, Val Loss: 0.0873, Val AUC: 0.9881, Val AUPRC: 0.9820\n",
      "Epoch: 122, Train Loss: 0.0668, Val Loss: 0.0886, Val AUC: 0.9881, Val AUPRC: 0.9820\n",
      "Epoch: 123, Train Loss: 0.0613, Val Loss: 0.0986, Val AUC: 0.9883, Val AUPRC: 0.9826\n",
      "Epoch: 124, Train Loss: 0.0648, Val Loss: 0.1070, Val AUC: 0.9885, Val AUPRC: 0.9833\n",
      "Epoch: 125, Train Loss: 0.0635, Val Loss: 0.1017, Val AUC: 0.9886, Val AUPRC: 0.9833\n",
      "Epoch: 126, Train Loss: 0.0597, Val Loss: 0.0920, Val AUC: 0.9887, Val AUPRC: 0.9830\n",
      "Epoch: 127, Train Loss: 0.0574, Val Loss: 0.0859, Val AUC: 0.9887, Val AUPRC: 0.9829\n",
      "Epoch: 128, Train Loss: 0.0586, Val Loss: 0.0838, Val AUC: 0.9889, Val AUPRC: 0.9831\n",
      "Epoch: 129, Train Loss: 0.0594, Val Loss: 0.0822, Val AUC: 0.9891, Val AUPRC: 0.9828\n",
      "Epoch: 130, Train Loss: 0.0594, Val Loss: 0.0826, Val AUC: 0.9894, Val AUPRC: 0.9830\n",
      "Epoch: 131, Train Loss: 0.0577, Val Loss: 0.0853, Val AUC: 0.9896, Val AUPRC: 0.9833\n",
      "Epoch: 132, Train Loss: 0.0604, Val Loss: 0.0847, Val AUC: 0.9894, Val AUPRC: 0.9830\n",
      "Epoch: 133, Train Loss: 0.0615, Val Loss: 0.0886, Val AUC: 0.9893, Val AUPRC: 0.9826\n",
      "Epoch: 134, Train Loss: 0.0612, Val Loss: 0.0914, Val AUC: 0.9892, Val AUPRC: 0.9822\n",
      "Epoch: 135, Train Loss: 0.0637, Val Loss: 0.0879, Val AUC: 0.9885, Val AUPRC: 0.9817\n",
      "Epoch: 136, Train Loss: 0.0586, Val Loss: 0.0872, Val AUC: 0.9882, Val AUPRC: 0.9817\n",
      "Epoch: 137, Train Loss: 0.0609, Val Loss: 0.0841, Val AUC: 0.9886, Val AUPRC: 0.9822\n",
      "Epoch: 138, Train Loss: 0.0624, Val Loss: 0.0809, Val AUC: 0.9896, Val AUPRC: 0.9824\n",
      "Epoch: 139, Train Loss: 0.0600, Val Loss: 0.0814, Val AUC: 0.9906, Val AUPRC: 0.9830\n",
      "Epoch: 140, Train Loss: 0.0605, Val Loss: 0.0858, Val AUC: 0.9905, Val AUPRC: 0.9827\n",
      "Epoch: 141, Train Loss: 0.0635, Val Loss: 0.0837, Val AUC: 0.9904, Val AUPRC: 0.9825\n",
      "Epoch: 142, Train Loss: 0.0586, Val Loss: 0.0797, Val AUC: 0.9903, Val AUPRC: 0.9824\n",
      "Epoch: 143, Train Loss: 0.0540, Val Loss: 0.0799, Val AUC: 0.9902, Val AUPRC: 0.9827\n",
      "Epoch: 144, Train Loss: 0.0580, Val Loss: 0.0798, Val AUC: 0.9901, Val AUPRC: 0.9834\n",
      "Epoch: 145, Train Loss: 0.0586, Val Loss: 0.0790, Val AUC: 0.9903, Val AUPRC: 0.9834\n",
      "Epoch: 146, Train Loss: 0.0567, Val Loss: 0.0819, Val AUC: 0.9906, Val AUPRC: 0.9834\n",
      "Epoch: 147, Train Loss: 0.0555, Val Loss: 0.0852, Val AUC: 0.9905, Val AUPRC: 0.9832\n",
      "Epoch: 148, Train Loss: 0.0567, Val Loss: 0.0824, Val AUC: 0.9905, Val AUPRC: 0.9832\n",
      "Epoch: 149, Train Loss: 0.0593, Val Loss: 0.0782, Val AUC: 0.9909, Val AUPRC: 0.9834\n",
      "Epoch: 150, Train Loss: 0.0583, Val Loss: 0.0768, Val AUC: 0.9910, Val AUPRC: 0.9834\n",
      "Epoch: 151, Train Loss: 0.0562, Val Loss: 0.0766, Val AUC: 0.9911, Val AUPRC: 0.9837\n",
      "Epoch: 152, Train Loss: 0.0587, Val Loss: 0.0765, Val AUC: 0.9911, Val AUPRC: 0.9837\n",
      "Epoch: 153, Train Loss: 0.0581, Val Loss: 0.0777, Val AUC: 0.9912, Val AUPRC: 0.9837\n",
      "Epoch: 154, Train Loss: 0.0566, Val Loss: 0.0811, Val AUC: 0.9912, Val AUPRC: 0.9837\n",
      "Epoch: 155, Train Loss: 0.0614, Val Loss: 0.0801, Val AUC: 0.9909, Val AUPRC: 0.9840\n",
      "Epoch: 156, Train Loss: 0.0564, Val Loss: 0.0798, Val AUC: 0.9905, Val AUPRC: 0.9844\n",
      "Epoch: 157, Train Loss: 0.0646, Val Loss: 0.0803, Val AUC: 0.9902, Val AUPRC: 0.9844\n",
      "Epoch: 158, Train Loss: 0.0547, Val Loss: 0.0807, Val AUC: 0.9903, Val AUPRC: 0.9846\n",
      "Epoch: 159, Train Loss: 0.0570, Val Loss: 0.0800, Val AUC: 0.9905, Val AUPRC: 0.9847\n",
      "Epoch: 160, Train Loss: 0.0571, Val Loss: 0.0789, Val AUC: 0.9908, Val AUPRC: 0.9847\n",
      "Epoch: 161, Train Loss: 0.0584, Val Loss: 0.0787, Val AUC: 0.9910, Val AUPRC: 0.9845\n",
      "Epoch: 162, Train Loss: 0.0568, Val Loss: 0.0790, Val AUC: 0.9911, Val AUPRC: 0.9845\n",
      "Epoch: 163, Train Loss: 0.0574, Val Loss: 0.0785, Val AUC: 0.9910, Val AUPRC: 0.9848\n",
      "Epoch: 164, Train Loss: 0.0555, Val Loss: 0.0779, Val AUC: 0.9910, Val AUPRC: 0.9847\n",
      "Epoch: 165, Train Loss: 0.0586, Val Loss: 0.0772, Val AUC: 0.9909, Val AUPRC: 0.9847\n",
      "Epoch: 166, Train Loss: 0.0584, Val Loss: 0.0771, Val AUC: 0.9907, Val AUPRC: 0.9846\n",
      "Epoch: 167, Train Loss: 0.0594, Val Loss: 0.0780, Val AUC: 0.9908, Val AUPRC: 0.9849\n",
      "Epoch: 168, Train Loss: 0.0550, Val Loss: 0.0806, Val AUC: 0.9910, Val AUPRC: 0.9851\n",
      "Epoch: 169, Train Loss: 0.0586, Val Loss: 0.0813, Val AUC: 0.9908, Val AUPRC: 0.9849\n",
      "Epoch: 170, Train Loss: 0.0572, Val Loss: 0.0769, Val AUC: 0.9908, Val AUPRC: 0.9850\n",
      "Epoch: 171, Train Loss: 0.0599, Val Loss: 0.0767, Val AUC: 0.9909, Val AUPRC: 0.9850\n",
      "Epoch: 172, Train Loss: 0.0573, Val Loss: 0.0776, Val AUC: 0.9911, Val AUPRC: 0.9852\n",
      "Epoch: 173, Train Loss: 0.0589, Val Loss: 0.0793, Val AUC: 0.9911, Val AUPRC: 0.9851\n",
      "Epoch: 174, Train Loss: 0.0567, Val Loss: 0.0794, Val AUC: 0.9911, Val AUPRC: 0.9850\n",
      "Epoch: 175, Train Loss: 0.0580, Val Loss: 0.0784, Val AUC: 0.9906, Val AUPRC: 0.9847\n",
      "Epoch: 176, Train Loss: 0.0553, Val Loss: 0.0789, Val AUC: 0.9904, Val AUPRC: 0.9846\n",
      "Epoch: 177, Train Loss: 0.0601, Val Loss: 0.0791, Val AUC: 0.9906, Val AUPRC: 0.9849\n",
      "Epoch: 178, Train Loss: 0.0574, Val Loss: 0.0824, Val AUC: 0.9910, Val AUPRC: 0.9857\n",
      "Epoch: 179, Train Loss: 0.0580, Val Loss: 0.0876, Val AUC: 0.9911, Val AUPRC: 0.9860\n",
      "Epoch: 180, Train Loss: 0.0657, Val Loss: 0.0803, Val AUC: 0.9904, Val AUPRC: 0.9854\n",
      "Epoch: 181, Train Loss: 0.0590, Val Loss: 0.0802, Val AUC: 0.9900, Val AUPRC: 0.9848\n",
      "Epoch: 182, Train Loss: 0.0590, Val Loss: 0.0801, Val AUC: 0.9901, Val AUPRC: 0.9850\n",
      "Epoch: 183, Train Loss: 0.0570, Val Loss: 0.0804, Val AUC: 0.9906, Val AUPRC: 0.9857\n",
      "Epoch: 184, Train Loss: 0.0599, Val Loss: 0.0853, Val AUC: 0.9907, Val AUPRC: 0.9858\n",
      "Epoch: 185, Train Loss: 0.0550, Val Loss: 0.0854, Val AUC: 0.9905, Val AUPRC: 0.9851\n",
      "Epoch: 186, Train Loss: 0.0599, Val Loss: 0.0802, Val AUC: 0.9903, Val AUPRC: 0.9851\n",
      "Epoch: 187, Train Loss: 0.0542, Val Loss: 0.0800, Val AUC: 0.9901, Val AUPRC: 0.9849\n",
      "Epoch: 188, Train Loss: 0.0581, Val Loss: 0.0796, Val AUC: 0.9902, Val AUPRC: 0.9849\n",
      "Epoch: 189, Train Loss: 0.0571, Val Loss: 0.0788, Val AUC: 0.9905, Val AUPRC: 0.9851\n",
      "Epoch: 190, Train Loss: 0.0552, Val Loss: 0.0803, Val AUC: 0.9907, Val AUPRC: 0.9852\n",
      "Epoch: 191, Train Loss: 0.0553, Val Loss: 0.0833, Val AUC: 0.9908, Val AUPRC: 0.9852\n",
      "Epoch: 192, Train Loss: 0.0563, Val Loss: 0.0813, Val AUC: 0.9910, Val AUPRC: 0.9853\n",
      "Epoch: 193, Train Loss: 0.0543, Val Loss: 0.0777, Val AUC: 0.9909, Val AUPRC: 0.9851\n",
      "Epoch: 194, Train Loss: 0.0574, Val Loss: 0.0772, Val AUC: 0.9907, Val AUPRC: 0.9849\n",
      "Epoch: 195, Train Loss: 0.0572, Val Loss: 0.0771, Val AUC: 0.9908, Val AUPRC: 0.9844\n",
      "Epoch: 196, Train Loss: 0.0546, Val Loss: 0.0778, Val AUC: 0.9910, Val AUPRC: 0.9843\n",
      "Epoch: 197, Train Loss: 0.0567, Val Loss: 0.0818, Val AUC: 0.9911, Val AUPRC: 0.9844\n",
      "Epoch: 198, Train Loss: 0.0587, Val Loss: 0.0870, Val AUC: 0.9911, Val AUPRC: 0.9847\n",
      "Epoch: 199, Train Loss: 0.0579, Val Loss: 0.0829, Val AUC: 0.9911, Val AUPRC: 0.9851\n",
      "Epoch: 200, Train Loss: 0.0604, Val Loss: 0.0789, Val AUC: 0.9909, Val AUPRC: 0.9855\n",
      "Epoch: 201, Train Loss: 0.0574, Val Loss: 0.0787, Val AUC: 0.9908, Val AUPRC: 0.9856\n",
      "Epoch: 202, Train Loss: 0.0579, Val Loss: 0.0785, Val AUC: 0.9908, Val AUPRC: 0.9857\n",
      "Epoch: 203, Train Loss: 0.0587, Val Loss: 0.0786, Val AUC: 0.9909, Val AUPRC: 0.9857\n",
      "Epoch: 204, Train Loss: 0.0561, Val Loss: 0.0803, Val AUC: 0.9911, Val AUPRC: 0.9862\n",
      "Early stopping triggered after 204 epochs due to no improvement in validation metric.\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.0778, Test AUC: 0.9911, Test AUPRC: 0.9857\n"
     ]
    }
   ],
   "source": [
    "epochs = 500 \n",
    "best_val_metric = 0.0 \n",
    "patience = 50 \n",
    "epochs_without_improvement = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train_link_prediction()\n",
    "    val_loss, val_auc, val_auprc = evaluate_link_prediction(\n",
    "        data['drug', 'causes', 'side_effect'].edge_index_val_pos,\n",
    "        data['drug', 'causes', 'side_effect'].edge_index_val_neg\n",
    "    )\n",
    "\n",
    "    current_val_metric = val_auc \n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val AUPRC: {val_auprc:.4f}')\n",
    "\n",
    "    # --- Early Stopping Logic ---\n",
    "    if current_val_metric > best_val_metric:\n",
    "        best_val_metric = current_val_metric\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch} epochs due to no improvement in validation metric.\")\n",
    "        break\n",
    "\n",
    "# --- Final evaluation on test set ---\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_auc, test_auprc = evaluate_link_prediction(\n",
    "    data['drug', 'causes', 'side_effect'].edge_index_test_pos,\n",
    "    data['drug', 'causes', 'side_effect'].edge_index_test_neg\n",
    ")\n",
    "print(f'Test Loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}, Test AUPRC: {test_auprc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
